\documentclass[11pt]{article}
\usepackage{float}
\usepackage{acl2014}
\usepackage{times}
\usepackage{url}
\usepackage{subfig}
\usepackage{graphicx}
\usepackage{latexsym}
\usepackage{slashbox}

%\setlength\titlebox{5cm}

\title{Clustering of Co-Occurring Neighboring Unambiguous Terms (COCONUT)}

\author{
Anouk Visser \\
{\tt 6277209}\\
  \\\And
  R\'emi de Zoeten \\
  {\tt 6308694}\\
   \\\And
  Cristina G\^arbacea \\
  {\tt 10407936}
  \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
In this paper we propose the COCONUT algorithm for unsupervised word sense disambiguation. We show how word co-occurrence vectors can be used to identify and classify distinct word senses, and quantify how distinct two word senses are. We build upon the findings of \cite{Mikolov:13} and show that COCONUT can improve accuracy when answering analogy questions.

\end{abstract}

\section{Introduction}
Recently \cite{Mikolov:13} found that continuous space word representations capture syntactic and semantic regularities. For example they find that $\textit{queen} - \textit{king} \approx \textit{woman}  -\textit{man}$. The authors use these linguistics regularities to answer a set of analogy questions of the form of `\textit{a} is to \textit{b} as \textit{c} is to \dots'. In their framework, every word is represented by exactly one continuous space word representation. One potential problem when answering analogy questions is that words can be ambiguous. When the analogy is `\textit{seed} is to \textit{apple} as \textit{window} is to \textit{house}' then the word apple refers to a fruit. Another example of an analogy is `\textit{apple} is to \textit{computer} as \textit{porsche} is to \textit{cars}' where the company apple is implied. It is clear that there are words that have two very distinct meanings. By representing words with just one vector (a continuous space representation), as done in \cite{Mikolov:13}, it is not possible to differentiate between the two different meanings of a word. We propose to disambiguate between the various senses of a word in order to obtain multiple continuous space word representations for one word. We will annotate a training corpus to give ambiguous words two different representations. Then we reproduce the steps described in \cite{Mikolov:13} to obtain a continuous space word representation for the different meanings of a word.

We present two different methods for word-sense disambiguation. In section \ref{remi} we describe how word-sense disambiguation can be accomplished by local co-occurrence clustering. In section \ref{anouk} we propose another method, COCONUT, that uses global co-occurrences to disambiguate words.

\section{Related Work}
The linguistic regularities in continuous space word representations used in \cite{Mikolov:13} can be identified by using a vector offset method based on cosine similarity. 

To obtain the continuous space word representations a recurrent neural network language model is used. A Recurrent Neural Networks (RNN) is a neural network that operates in time. At each timestep it accepts an input vector and by the use of nonlinear activation functions it updates its (high dimensional) hidden state to make a prediction of the output. RNNs are regarded as a rich class model because high dimensional distributed representations can be stored inside their hidden state. In addition, they can implement very complex computations and can perform modelling and prediction tasks for sequences with a highly rich structure.
%XXX The RNN is trained using backpropagation to maximize data likelihood and consists of one input layer that accepts one word at a time encoded using \textit{1}-of-\textit{N} encoding scheme, an output layer which outputs a probability distribution over possible words and a hidden layer with recurrent connections that keeps track of the sentence history.- MAYBE JUST GIVE THE IDEA OF RNNs, INSTEAD OF TELLING THIS. XXX

To answer the question `$a$ is to $b$ as $c$ is to \dots', we use the word vectors $x_a, x_b, x_c$ to determine the word which is assumed to be the best answer to a question. Following the example from the introduction, $\textit{queen} - \textit{king} \approx \textit{woman}  -\textit{man}$, we find that the answer to the question must be $y = x_b - x_a + x_c$. Because it is unlikely that this exact $y$ exists, we find the answer to the question by maximizing the cosine similarity between y and the words in the corpus. We can also use this method in order to define a similarity between word pairs $a$:$b$ and $c$:$d$. In this case we can answer less obvious analogy questions, we might for example want to find the relationship of competitors as denoted by the word pair \textit{apple}:\textit{dell}. When we are given two new word pairs \textit{china}:\textit{nike} and \textit{ford}:\textit{mercedes} we can rank them by computing the relational similarity: $cos(x_b - x_a + x_c, x_d)$.\\\\
Linguistic regularities can be used in many different unsupervised language learning applications. An example is presented by \cite{MikolovMT:13}, where a method is proposed to exploit similarities among languages for machine translation. For machine translation some sense of a dictionary or phrase table is required. However, these are not always available, or are incomplete. By using two monolingual corpora a model can be trained and linguistic regularities can be learned for the two different languages. If the dictionary entry for \textit{queen} is missing, but, for example, the entry for \textit{king} is available, we can find the translation for \textit{queen} by using the vector offset method as described above.
\section{Word-sense disambiguation by local co-occurrence clustering}
\label{remi}
\begin{figure}
\center
\subfloat[]{
	\includegraphics[scale=0.25]{inc.png}
}

\subfloat[]{
	\includegraphics[scale=0.25]{pie.png}
}
\caption{In these two examples for the window around `apple' the black words will be count as co-occurring words, whereas the grey words will be ignored.}
\label{window}
\end{figure}
To find the two senses of a word, one approach would be to cluster different meanings of a word based on the words that it co-occurs with in the corpus. This is done by generating a co-occurrence vector for every time the word is observed in the corpus (this is a local co-occurrence vector). A co-occurrence vector is derived by observing the context of a word. In our experiments the frequency of the words that fall within a window of 5 words from the word that is being observed are encoded into the vector, an illustration can be found in figure \ref{window}. This implies that each vector is a sparse vector with the length of the vocabulary size, but can be encoded with at most 10 terms. These co-occurrence vectors are then clustered using k-means clustering. It is possible (even likely) that two co-occurrence vectors do not have a word in common, but still end up in the same cluster. For example, in the case of apple the words \{\textit{technology, iphone, company, revenue}\} might be in the same cluster. Given the co-occurrence vectors, 1:\{\textit{technology, iphone}\}, 2:\{\textit{iphone, revenue}\}, 3:\{\textit{technology, company}\} and 4:\{\textit{company, revenue}\} then 1 and 4 have nothing in common, but can still be bound together by 2 and 3. It should be noted that extracting all co-occurrence vectors from a corpus can require a significant amount of memory, even when using sparse-vector encoding. However, it is possible to have a fine-tunable tradeoff between memory requirements and the number of loops over the corpus (which is more cpu-intensive), by only recording a specific subset of the vocabulary on each iteration.
\section{COCONUT}
\label{anouk}
The COCONUT method for disambiguating words is based on two assumptions: 
\begin{enumerate}
\item the meaning of a word is highly dependent on the words accompanying it
\item the co-occurring words that define one meaning of a word are more likely to co-occur with each other than two words that define two different meanings of the word
\end{enumerate}
In assumption $(1)$ we talk about `words that accompany a word'. In COCONUT, words that accompany a word are co-occurring words. COCONUT will split the co-occurrence vector for \textit{apple} into two co-occurrence vectors, one containing \textit{technology}, \textit{company} and \textit{iPhone}, the other containing \textit{fruit}, \textit{baking} and \textit{pie}. 

An example to further explain assumption (2) can be found in figure \ref{cococ}. Here one meaning of the word \textit{apple} can be characterized by the words \textit{technology}, \textit{company} and \textit{iPhone}, while the other can be characterized by the words \textit{fruit}, \textit{baking} and \textit{pie}. We hypothesize that two words describing one meaning of \textit{apple}, for example, \textit{iPhone} and \textit{technology} are more likely to occur together than two words describing two different meanings, for instance, \textit{iPhone} and \textit{baking}.

Let $C$ be the set of words that co-occur with $A$, the word we want to disambiguate. COCONUT first constructs and converts the global co-occurrence vectors of the words in $C$ to relatedness vectors. It will then cluster these relatedness vectors in order to determine the two possibly different meanings of $A$. 
\begin{figure}
\center
	\includegraphics[scale=0.20]{cococ.png}
\caption{Representation of the co-occurrence vectors of the words that co-occur with \textit{apple}. This is just for illustration purposes, neither the co-occurring words or the relatedness scores were obtained from an actual dataset. The grey areas show the co-occurrence vector for the word \textit{apple}. When looking at the co-occurring words, we find that the co-occurring words that describe one meaning of \textit{apple} are more likely to co-occur with each other. The purple area shows that the words \textit{fruit}, \textit{baking} and \textit{pie} are pretty related to each other, whereas they are not so related to \textit{technology}, \textit{company} and \textit{iPhone} (blue area). We observe a similar pattern for the green and orange areas.}
\label{cococ}
\end{figure}

\subsection{Co-Occurrence Vectors}
A global co-occurrence vector contains the frequencies indicating how many times two words co-occur. We obtain the global co-occurrence vector for every word in the corpus in a similar way to how we obtaining the local co-occurrence vector as described in section \ref{remi}. The only difference being that instead of maintaining all local co-occurrence vectors for a given word, we accumulate all local co-occurrence vectors in one global co-occurrence vector. This significantly reduces memory requirements which makes the COCONUT algorithm much more usable. 

After obtaining the global co-occurrence vectors for every word in the corpus, we convert the absolute frequencies in the global co-occurrence vector to a relatedness score. We use the same function for relatedness as \cite{Guthrie:92}:
$$r(x, y) = \frac{f_xy}{f_x+f_y - f_{xy}}$$
where $f_{xy}$ denotes the frequency of $x$ and $y$ occurring together and $f_x$ and $f_y$ denote the frequency of $x$, respectively $y$. Words that are not closely related to $A$ do not contribute to either one of the meanings. Therefore, we will discard the words that have a relatedness score with $A$ that falls in the bottom $50\%$ of all relatedness-scores from $C$. The terms that are discarded are considered relevant to all meanings of $A$, we will call this set $R$.

\subsection{Clustering and splitting}
Let the set of co-occurrence vectors from the words in $C$, be called $V$. After applying k-means clustering on the vectors in $V$ we expect to find two cluster centers that represent the two meanings for $A$. Note that we are only interested in describing the two meanings of $A$ using the words in $C$. Therefore, for every vector in $V$ we will discard all words that are not in $C$. The adjusted vectors can now be used to perform k-means clustering. 

The two new co-occurrence vectors for $A$ are initialized with the words in $R$. As the cluster centers define the different meanings of $A$, we can look at the words in each cluster to fill the new co-occurrence vectors for $A$. For example if the words `technology', `iPhone' and `company' are assigned to one cluster, they will be inserted into one of the new co-occurrence vectors for the word `apple' while the words `fruit', `pie', `baking' that were assigned to the other cluster will be inserted into the other co-occurrence vector. 

COCONUT will split every word in the corpus in order to find two different meanings (we excluded the 75 most frequent words), but not all words are ambiguous. We expect that words that have two distinct meanings will have a greater cluster distance (i.e. a greater distance between the two meanings) than words that do not. We discard all disambiguations for the words that have a cluster distance that falls in the bottom $50\%$ of all cluster distances.

\section{Corpus annotation and question answering.}
In our experiments we allowed every word to be split into either one or two different meanings.
Once a set of disambiguated words and their representation has been identified, a new corpus is generated wherein the ambiguous words are annotated with their meaning. This is done by looping over the words in the corpus and again extracting the context of the words that are ambiguous. This context is then matched with one of the clusters that were found for that particular word. We annotate a word with the index of the cluster that best describes its meaning. Then, the process described in \cite{Mikolov:13} is repeated to get the word-vector representation of the words in the annotated corpus. \\
It might not be desirable to split every word in the vocabulary. For example, it might not be informative to split stop words like \textit{the, and, a, an}. That is why in our experiments we did not split the 75 most frequent words. We also do not want to split words for which we do not have sufficient data. We addressed this by only splitting words that occur more than $20$ times in the corpus. Furthermore, some words might be clustered into very distinct representations, and some might produce more similar clusters. We ordered the clustered words according to the cosine similarity of their distinct clusters, and only split a certain fraction of the words in the vocabulary for which the cluster distances were the greatest. We have done measurements for splitting $0$, $\frac{1}{3}$ and $\frac{1}{10}$ of the words in our vocabulary and evaluated the performance of each in \ref{qual}.\\
Now the question triplet `$a$ is to $b$ as $c$ is to \dots' can be translated into many interpretations, namely `$a\_0$ is to $b\_0 $ as $c\_0 $ is to \dots', `$a\_1$ is to $b\_0 $ as $c\_0 $ is to \dots', etc. If all three words are ambiguous then there are 8 possible interpretations. Of course, not all interpretations are sensible. If the question is `\textit{king} is to \textit{queen} as \textit{man} is to \dots' then the interpretation of \textit{queen} as a band is not sensible, but should be interpreted as `queen as-in royalty'. To achieve this result we first ask the question `which a and which b are most similar?' This is done by combining all senses of \textit{a} and all senses of \textit{b} and measuring their distance. It is assumed that royalty king and royalty queen are closer together than, for example, card-game king and the band queen. We have now reduced the number of questions to only two questions, where \textit{c} is still ambiguous. Now both questions will be answered and an error $\mid\mid \textit{e} \mid\mid$ is determined for each answer, such that $\textit{b} - \textit{a} + \textit{c} \equiv \textit{answer} + \textit{e}$. Finally we choose the answer with the smallest error.

\section{Evaluation}
We have evaluated the performance of COCONUT on the \textit{enwiki8}\footnote{http://cs.fit.edu/mmahoney/compression/textdata.html} dataset containing a total of 12577300 words and 60237 unique words. 

We performed an empirical evaluation in which we inspect the two meanings of a word. We also did a quantitative evaluation by using the continuous space word representations to answer a number of analogy questions. Different sets of analogy questions are available. For example the 8000 analogy questions used in \cite{Mikolov:13} contain adjectives (big, bigger, rough, rougher), noun (car, cars, apple, apples) and verb (avoid, avoids, wait, waits) questions. There are also many other analogy questions such as man-woman relation ships, country-currency relationships and more.

\subsection{Similarity Measures}
\begin{figure}[H]
\center
    \begin{tabular}{l|l}
    \textbf{Similarity Metric} & \textbf{Accuracy}  \\ \hline
    Cosine            & 16.20     \\ \hline
    Euclidean         & 16.17     \\ \hline
    Manhattan         & 16.49     \\
    \end{tabular}
    \caption{Results for various word split fractions.}
    \label{metric1}
\end{figure}

In \cite{Mikolov:13} the cosine similarity measure is used to find $y$ in $y = b - a + c$. We have tried some other similarity measures to see whether these would improve the accuracy on the set of 8000 syntactic analogy questions proposed by the authors. Measuring the similarity between two vectors can be seen as an equivalent to measuring their distance. Inversion or subtraction can easily be applied to transform a measure of distance between vectors into a measure of similarity. We report the accuracy for the cosine similarity as well as the euclidean distance and the manhattan distance, using the 80-dimensional word projections from \cite{Mikolov:13} that can be found online\footnote{http://rnnlm .org}. Figure \ref{metric1} shows the results for the different similarity measures. Although the manhattan distance slightly outperformed the cosine similarity we have chosen to use cosine similarity because the differences are not very significant.

\subsection{Empirical Evaluation}
For the empirical evaluation we have inspected the results of COCONUT and local co-occurrence clustering on a small hand-made dataset. The results for both methods are very similar and we only present our evaluation for COCONUT, because it is the only algorithm that can scale to larger datasets, for which a quantitative evaluation is described in section \ref{qual}. The hand-made dataset used for empirical evaluation contains different fragments of wikipedia articles on ambiguous topics. It includes apple (fruit, company), queen (band, monarch), jaguar (company, animal), eagles (band, animal), firm (law firm, firm grip), range (of numbers, farm fields) and more. For every ambiguous word we also took fragments from their superclasses. In addition to articles revolving around the ambiguous words, we also added some random articles.

The results obtained on this small dataset were promising, for example the words from two different clusters  (sorted based on relatedness) for the word `apple' are: 
\begin{enumerate}
\item also, fruit, june, announced, crisp, pie, crumble, inc, 9, is, apples, such, jelly, pomaceous, cake, 77, butter, processor, juice
\item iphone, wwdc, operating, develops, on, x, os, are, 4, sauce, desserts, nokia, remote, towards, offers, system, largest, worlds
\end{enumerate}
Another example is `jaguar' for which we find: 
\begin{enumerate}
\item feline, fords, under, dropped, solitary, enjoys, waters, threatened, preferred, sustained, inland, 59, rainforest, swimming, across, ownership, largely, exceptionally, planned
\item models, sported, has, plated, traditionally, chrome, prominently, forming, famous, changed, hunts, grounds, associated, featured, americas, fishing
\end{enumerate}
There is a fair amount of noise in the different clusters, but overall there is a reasonably clear distinction between the two different meanings of these words.
On the \textit{enwiki8} dataset we find a lot more noise in the two clusters, including a lot of words that are relevant to both clusters. When inspecting the clusters we noticed that the words with the highest relatedness to the disambiguated word were most likely to be correct. However, the majority of the words that show little relatedness (even if they are in the top $50\%$ of related words) do not describe the meaning as well. We now provide two examples of clusters formed on the \textit{enwiki8} dataset:\\\\
`santa'
\begin{enumerate}
\item claus, maria', monica, clara, tenerife, ana, san, croce, christmas
\item cruz, fe, barbara, catarina, grande, california, marta, mar, del
\end{enumerate}
We observe a lot of noise in this example. However, the meaning of `santa'-as-in Christmas is captured in meaning 1, whereas the meaning of `santa'-as-in location is captured in meaning 2. \\\\
`belief'
\begin{enumerate}
\item god, faith, knowledge, justification, jesus, religion, absence, afterlife, resurrection
\item contrary, justified, atheism, systems, deities, beliefs, lack, freedom, feminism
\end{enumerate}
Meaning 1 is more centered around `belief'-as-in religion, whereas meaning 2 is more centered around a multitude of beliefs. 

\subsection{Quantitative Evaluation}
\label{qual}
For our quantitative evaluation we trained on the enwiki8 dataset. We evaluated on the set of $8.000$ questions from \cite{Mikolov:13}, which are syntactic questions and also evaluated on some question sets described in \cite{Mikolov:13c}. In our experiments we only split a fraction of the vocabulary, for which the cluster distances were greatest. Splitting a fraction of $0$ words therefore serves as a baseline.
\begin{table}[H]
    \begin{tabular}{|l|l|l|l|}
    \hline
	\backslashbox{Questions}{Split} & 0 & $\frac{1}{10}$ & $\frac{1}{3}$  \\ \hline
	Syntactic  & \textbf{11.86}  & 11.26  & 10.99      \\ \hline
	Family  & 32.61  & 33.60 & \textbf{34.19}     \\ \hline
	Plurals  & \textbf{21.85}  & 19.00 & 21.02     \\ \hline
	Plural Verbs  & 8.16 & 9.20 & \textbf{10.11}     \\ \hline
	Nationality adjectives  & 23.51 & \textbf{25.58} & 25.32     \\ \hline
	Opposites  & \textbf{5.42} & 4.43 & 3.94    \\ \hline

    \end{tabular}
     \caption{Results for answering analogy questions using 80-dimensional word projections. Numbers indicate \% correctly answered questions.}
    \label{accuracy80}
\end{table}

\begin{table}[H]
    \begin{tabular}{|l|l|l|l|}
    \hline
	\backslashbox{Questions}{Split} & 0 & $\frac{1}{10}$ & $\frac{1}{3}$  \\ \hline
	Syntactic  & 13.48  & \textbf{13.85}  & 13.175      \\ \hline
	Family  & 36.76  & 34.58 & \textbf{37.94}     \\ \hline
	Plurals Nouns & 24.10  & \textbf{26.65} & 23.35     \\ \hline
	Plural Verbs  & \textbf{15.75} & 11.72 & 11.96     \\ \hline
	Nationality adjectives  & 32.83 & \textbf{37.46} & 33.52     \\ \hline
	Opposites  & \textbf{7.88} & 6.77 & 7.51    \\ \hline

    \end{tabular}
     \caption{Results for answering analogy questions using 320-dimensional word projections. Numbers indicate \% correctly answered questions.}
    \label{accuracy320}
\end{table}

As can be seen in tables \ref{accuracy80} and \ref{accuracy320} the COCONUT results improve more compared to the baseline results when larger word vector representations are used. At the same time, it  also appears that splitting a smaller fraction of the vocabulary leads to better results on larger word vector representations, while at the same time, having no split does not perform best.

In order to gain more insight in the distribution of the cluster distances we plotted \ref{dist} and \ref{distN}. We measured the cluster distances as $1-cos(a,b)$ where \textit{a} and \textit{b} are length normalized vectors that define a cluster center.
 In the distribution showed in \ref{dist} all words carry equal weight, regardless of their frequency in the corpus. When the frequency of cluster distances is adjusted for word frequency in the corpus the distribution looks different, as can be seen in \ref{distN}. Graphs \ref{dist} and \ref{distN} suggest that frequent words are less likely to have a large cluster distance. This difference indicates that ambiguous words are less frequent than unambiguous words in the enwiki8 corpus.

\section{Discussion and future work}
The results presented in \ref{qual} might be somewhat sparse given the number of variables. In future work we would like to discover more about the relationship between split fraction and word vector size.

\subsection{Co-occurrence vectors}
For our experiments we have used a window of 5 words to determine the co-occurrence vector, where each of the 10 words that fall in this window are weighted equally. It is also possible to use a soft-max or gaussian measure to weigh each word based on its distance.

\subsection{Detecting ambiguousness}
One of the most challenging problems in word-sense disambiguation is deciding whether a word is ambiguous or not. We have discard all disambiguations that have a cluster distance that fall outside a certain split. In future work, we might want to focus on finding a better way of detecting whether a word is ambiguous or not. 
In our work we have assumed that a word can have at most two meanings. This is not a very solid assumption, we find many difference meanings for `bank' including 1) financial institute 2) land alongside a river 3) cushion of a pool table. While inspecting the clusters, we found a lot of words that we would not consider ambiguous might occur in many different contexts, slightly changing the meaning of a word. An example of a word we found that has many different meanings is `red'. We provide a limited breakdown of the words that co-occur with `red':
\begin{itemize}
\item Boston Red Sox - sports
\item Red Sea, Red Square - places
\item Communism, love - concepts
\item Relief, Red Cross, red label whisky - non-profits / brands
\end{itemize} 

We believe that splitting the word vector for `red' should still boost the performance when answering analogy questions. The analogy question `\textit{red} is to \textit{sox} as \textit{blue} is to \dots' might be difficult to answer when all different meanings of the word `red' are encoded into one word representation. Once we are able to disambiguate the word `red' we find a word representation that is focused at sports and not so much at communism. (the answer to `\textit{red} is to \textit{sox} as \textit{blue} is to \dots' would be `jays' as in Toronto Blue Jays, another question we could ask is `\textit{red} is to \textit{boston} as \textit{blue} is to \dots')

\begin{figure}
	\center
	\subfloat[]
	{
		\includegraphics[scale=0.35]{distances_step500.png}
		\label{dist1}
	}\newline
	\subfloat[]
	{
		\includegraphics[scale=0.35]{distances_normed_cumulative.png}
		\label{dist2}
	}
	\caption{Distribution over cluster distances as they occur in the enwiki8 vocabulary. \ref{dist1} shows the distribution over cluster distances and \ref{dist2} shows the cumulative distribution over cluster distances. (\textit{y} axis are proportional to to the vocabulary size, \textit{x} indicates cluster distance)}
	\label{dist}
\end{figure}

\begin{figure}
	\center
	\subfloat[]
	{
		\includegraphics[scale=0.35]{weighted_distances.png}
		\label{distN1}
	}\newline
	\subfloat[]
	{
		\includegraphics[scale=0.35]{weighted_cumulative_normed.png}
		\label{distN2}
	}
	\caption{Distribution over cluster distances as they occur in the enwiki8 corpus. \ref{distN1} shows the distribution over cluster distances and \ref{distN2} shows the cumulative distribution over cluster distances. (\textit{y} axis are proportional to to the vocabulary size, \textit{x} indicates cluster distance)}
	\label{distN}
\end{figure}
\pagebreak

\section{Conclusion}
We proposed the COCONUT algorithm for unsupervised word sense disambiguation and demonstrated that pre-processing a corpus using COCONUT can improve accuracy of answering analogy questions. Not only can COCONUT be used for word disambiguation, it can also quantify how distinct two senses of a word are. 


\begin{thebibliography}{}
\bibitem[\protect\citename{Guthrie \bgroup et al.\egroup }1991]{Guthrie:92}
Joe A. Guthrie, Louise Guthrie, Yorick Wilks and Homa Aidinejad.
\newblock 1991.
\newblock Subject-dependent co-occurrence and word sense disambiguation.
\newblock Proceedings of the 29th annual meeting on Association for Computational Linguistics, 
146--152
\newblock
Association for Computational Linguistics

\bibitem[\protect\citename{Mikolov \bgroup et al.\egroup }2013a]{Mikolov:13}
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
\newblock 2013a.
\newblock Linguistic regularities in continuous space word representations.
\newblock Proceedings of NAACL-HLT, 
746--751

\bibitem[\protect\citename{Mikolov \bgroup et al.\egroup }2013b]{MikolovMT:13}
Tomas Mikolov, Quoc V. Le and Ilya Sutskever.
\newblock 2013b.
\newblock Exploiting Similarities among Languages for Machine Translation.
\newblock arXiv preprint arXiv:1309.4168, 

\bibitem[\protect\citename{Mikolov \bgroup et al.\egroup }2013c]{Mikolov:13c}
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.
\newblock 2013c.
\newblock Efficient Estimation of Word Representations in Vector Space.
\newblock Proceedings of Workshop at ICLR

\end{thebibliography}

\end{document}
