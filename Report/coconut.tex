%
% File acl2014.tex
%
% Contact: koller@ling.uni-potsdam.de, yusuke@nii.ac.jp
%%
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn, 
%% based on the style files for ACL-2010, which were, in turn, 
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{acl2014}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}

%\setlength\titlebox{5cm}

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.


\title{Clustering of Co-Occurring Neighboring Unambiguous Terms (COCONUT)}

\author{Anouk Visser \\
{\tt anouk.visser@student.uva.nl}\\
  \\\And
  R\'emi de Zoeten \\
  {\tt remi.de.z@gmail.com}\\
   \\\And
  Cristina G\^arbacea \\
  {\tt cr1st1na.garbacea@gmail.com}
  \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
Let's decide on the abstract once we finish the body of the text.
  %Vector space models for word representation have shown to be useful in capturing the relationships between words' functions and meanings. Similarities between words are encoded under the form of distance or angle in a high dimensional space. Neural language models, although less used than the traditional \textit{n}-gram models because of their notoriously long training times, present superior performance on the task of word prediction. Leaving from the work of \cite{Mikolov:13}, we propose three new methods for word sense disambiguation based on the co-occurence frequency of the context words near a given target word. We show that these are valid approaches in an unsupervised setting and can increase the accuracy of capturing syntactic and semantic regularities for the English language.
\end{abstract}

\section{Introduction}
The introduction will be here.
% What will we be doing? 
% Why are we doing this? (Ambiguous words must get different projections).

\section{Related Work}
Recently \cite{Mikolov:13} found that continuous space word representations capture syntactic and semantic regularities. For example they find that $\textit{queen} - \textit{king} + \textit{man} \approx \textit{woman}$. The authors use these linguistics regularities to answer a set of analogy questions in the form of `$a$ is to $b$ as $c$ is to \dots'. In addition to this, the linguistic regularities can also be used to compute similarity of the relations of pairs of words $a:b$ and $c:d$.

The linguistic regularities in continuous space word representations can be identified by using a vector offset method based on cosine similarity. A recurrent neural network language model is used to obtain the continuous space word representations. The RNN is trained using backpropagation to maximize data likelihood and consists of one input layer that accepts one word at a time encoded using \textit{1}-of-\textit{N} encoding scheme, an output layer which outputs a probability distribution over possible words and a hidden layer with recurrent connections that keeps track of the sentence history. The embedding vectors $x_a, x_b, x_c$ are used to determine the word which is assumed to be the best answer to a question, $y = x_b - x_a + x_c$, or in case there is no word in space at this position, the word having the greatest cosine similarity with $y$. In the case of semantic evaluation where $d$ is given, computing $cos(x_b - x_a + x_c, x_d)$ determines the measure of relational similarity between the prototypical and target word pairs.

There is a huge potential for using linguistic regularities in unsupervised language learning applications. An example application is presented by \cite{MikolovMT:13}, where a method is proposed to exploit similarities among languages for machine translation. For machine translation some sense of a dictionary or phrase table is required. However, these are not always available, or are incomplete. By using two monolingual corpora a model can be trained and linguistic regularities can be learned for the two different languages. If the dictionary entry for `queen' is missing, but, for example, the entry for `king' is available, we can find the translation for `queen' by using the vector offset method as described above.
% Talk about RNNs
% Talk about Linguistic Regularities (as that is pretty related), vector offset model
% also talk about that MT paper...

\section{Word disambiguation by word-level co-occurrence clustering}
One potential problem when answering analogy questions is that words can be ambiguous. When the question is \textit{Apple is to XXX what XXX is to \dots} then the word apple refers to a fruit. Another question could be \textit{Apple is to XXX what XXX is to \dots} where the company apple is implied. Clearly, these are two very distinct entities, but the approach that Mikolov et al. presented does not differentiate. It would be appropriate to disambiguate between the various senses of the word apple, before answering the question. One approach is to cluster different meanings of a word based on the words that it co-occurs with in the corpus. This is done by generating a co-occurs vector for every time the word is observed in the corpus. The co-occurrence vector is derived by observing the context of a word. In our experiments the frequency of the words that fall within a window of 5 words from the word that is being observed are encoded into the vector. This means that each vector is a sparse vector with the length of the vocabulary size, but can be encoded with at most 10 terms. It would also be possible to use a soft-max or gaussian measure to weigh each word based on its distance, but we have weighted each of the 10 words in the 5 word window equally. These co-occurrence vectors are then clustered using k-means clustering. It is possible (even likely) that two co-occurrence vectors have no word in common, but still end up in the same cluster. For example, in the case of apple the words \{\textit{technology, iphone, company, revenue}\} might be in the same cluster. Given the co-occurrence vectors, 1:\{\textit{technology, iphone}\}, 2:\{\textit{iphone, revenue}\}, 3:\{\textit{technology, company}\} and 4:\{\textit{company, revenue}\} then 1 and 4 have nothing in common, but can still be bound together by 2 and 3. It should be noted that extracting all co-occurrence vectors from a corpus can can require a significant amount of memory, even when using sparse-vector encoding. However, it is possible to have a fine-tunable tradeoff between memory requirements and the number of loops over the corpus (which is more cpu-intensive), by only recording a specific subset of the vocabulary on each iteration.


\section{COCONUT}
% Describe intuition
For learning the word representations \cite{Mikolov:13} train an RNN with co-occurrence vectors of words. Instead of representing words by just one co-occurrence vector, we propose to train the model with multiple co-occurrence vectors for ambiguous words. The meaning of the word 'apple' can be determined by looking at its surrounding words, which could be: technology, iPhone, company for `Apple', the company or: fruit, orchard, pie for `apple' the fruit. COCONUT assumes that the meaning of a word is highly dependent on the words that accompany it and that the co-occurring words that define one meaning of `apple' are more likely to co-occur with each other than two words that define two different meanings of apple (`iPhone' and `technology' are more likely to occur together than `iPhone' and `orchard'). COCONUT will attempt to split the co-occurrence vector for `apple' into two co-occurrence vectors, one containing `iPhone', `technology' and `company', the other containing `fruit', `orchard' and `pie'.
% Let A denote the word we want to disambiguate

\subsection{Co-Occurrence Vectors}
We construct the co-occurrence vector for word $A$ by computing the relatedness of word $A$ with every other word in the vocabulary. We use the same function for relatedness as \cite{Guthrie:92}:
$$r(x, y) = \frac{f_xy}{f_x+f_y - f_xy}$$
where $f_xy$ denotes the frequency of $x$ and $y$ occurring together and $f_x$ and $f_y$ denote the frequency of $x$, respectively $y$. % something about how we get f_xy (RŽmi?)


\subsection{Clustering}
To find the two senses of a word, we apply k-means clustering to the co-occurrence vectors of the co-occurring words. COCONUT assumes that the words assigned to each cluster represent a different meaning of a word. Words that are not closely related to $A$ do not contribute to either one of the meanings. Therefore, we will not use the co-occurrence vectors of all co-occurring words, but only those from the words that are closely related. Building a good decision process for defining when a word is closely related to another word is beyond the scope of this project and will most likely not necessarily lead to significant performance improvements. Therefore, we have decided to discard the words that have a relatedness score with $A$ that falls in the bottom $50\%$ of all relatedness-scores. Let the set of words that remains be called $C$. We can use the co-occurrence vectors of the words in $C$ to find clusters, but these vectors will contain a lot of words that are not in $C$, do not occur together with $A$ or do occur with $A$ but not in $C$. We are only interested in finding clusters representing the different meanings of word $A$, therefore we will only use the co-occurring words in the vectors of $C$ that are present in $C$.
% some tweaks

\section{Corpus annotation and question answering.}

\section{Latent Semantic Analysis}
Unsupervised word sense disambiguation approaches exploit the idea that similar senses of a word have similar neighboring words. They try to induce word senses from input text by clustering word co-occurences, aiming to divide "the occurrences of a word into a number of classes by determining for any two occurrences whether they belong to the same sense or not" \cite{Schutze:98}.

\section{Similarity Measures}
Measuring the similarity between two vectors can be seen as an equivalent to measuring their distance. Inversion or subtraction can be easily applied to transform a measure of distance between vectors into a measure of similarity. 

The most common way to measure similarity between two vectors is to compute the \textit{cosine} of the angle between them as the inner product of the two vectors, after they have been normalized to unit length: $cos(x,y) = \frac{x \cdot y}{||x||\cdot||y||}$. Hence the length of the vectors is irrelevant. \cite{Bullinaria:97} show that the cosine is highly reliable and performs the best, after having compared it with distance measures like Hellinger, Bhattacharya, and Kullback-Leibler. Other common geometric metrics frequently used in the vector space are represented by the Euclidean, Manhattan and Mahalanobis distance, Dice, Jaccard, Pearson and Spearman correlation coefficients.

The \textit{Euclidean} distance between two points is defined as the length of the line connecting them. In the vector space, it is defined as $d(p,q) = d(q,p) = \sqrt{\sum_{i=1}^{n}(q_i-p_i)^2} $. The smaller this distance the more similar the objects are.  In a similar manner, the \textit{Manhattan} distance is defined as the sum of the absolute differences of the coordinates of two given points as $d(p,q) = |\sum_{i=1}^{n}(p_i - q_i)|$. The \textit{Mahalanobis} distance generalizes the standard Euclidean distance by modelling the relations of elements in different dimensions. Given two vectors $x$ and $y$, their squared Mahalanobis distance is $d_A = (x-y)^T A (x-y)$, where $A$ is a positive semidefinite matrix.

The \textit{Pearson} correlation coefficient is defined in a similar manner with the Spearman correlation coefficient, with the mention that the last one is between the ranked variables: $\rho=\frac{\sum_i(x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_i(x_i - \bar{x})^2 (y_i - \bar{y})^2 }}$, where $\bar{x} = \frac{\sum_{i=1}^{n}x_i}{n}$ and $\bar{y} = \frac{\sum_{i=1}^{n}y_i}{n}$. A value of 1 indicates total positive correlation, i.e. that a "best fit" line with a positive slope is generated which runs through all the datapoints, a value of 0 means no correlation and a value of -1 represents negative correlation between the two variables. The main advantage of this method over the Euclidean distance is that it is more robust against data which is not normalized.

\section{Evaluation}
We have evaluated the performance of COCONUT on a dataset containing X unique words, and has size X. Initially, we decided not to disambiguated the top X words, after extracting the two senses of the words and their distance, we discarded half of the disambiguated words, leaving us with X words that were disambiguated. 
% Description of the questions and answers?

\subsection{Empirical Evaluation}
%Maybe we need some sort of empirical evaluation in which we show how the clustering really works (and how it fails). 


\subsection{Quantitative Evaluation}
% Which test sets are we using? What are we comparing against? Some (but not many) statistics of our dataset.

\section{Conclusion}
\begin{thebibliography}{}

\bibitem[\protect\citename{Mikolov \bgroup et al.\egroup }2013a]{Mikolov:13}
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
\newblock 2013a.
\newblock Linguistic regularities in continuous space word representations.
\newblock Proceedings of NAACL-HLT, 
746--751

\bibitem[\protect\citename{Mikolov \bgroup et al.\egroup }2013b]{MikolovMT:13}
Tomas Mikolov, Quoc V. Le and Ilya Sutskever.
\newblock 2013b.
\newblock Exploiting Similarities among Languages for Machine Translation.
\newblock arXiv preprint arXiv:1309.4168, 

\bibitem[\protect\citename{Guthrie \bgroup et al.\egroup }1991]{Guthrie:92}
Joe A. Guthrie, Louise Guthrie, Yorick Wilks and Homa Aidinejad.
\newblock 1991.
\newblock Subject-dependent co-occurrence and word sense disambiguation.
\newblock Proceedings of the 29th annual meeting on Association for Computational Linguistics, 
146--152
\newblock
Association for Computational Linguistics

\bibitem[\protect\citename{Bullinaria \bgroup et al.\egroup }1997]{Bullinaria:97}
John Bullinaria and John Levy.
\newblock 1997.
\newblock Extracting semantic representations from word co-occurrence statistics: A computational study.
\newblock Behaviour Research Methods, 
510--526

\bibitem[\protect\citename{Schutze \bgroup et al.\egroup }1997]{Schutze:98}
Schutze H.
\newblock 1998.
\newblock Automatic Word Sense Discrimination.
\newblock Computational Linguistic, 
97
\end{thebibliography}

\end{document}
